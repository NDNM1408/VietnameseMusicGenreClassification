{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952d1af3-626e-44ce-b47b-89aa56b4c977",
   "metadata": {},
   "source": [
    "# VietNamese Music Classification\n",
    "## 1. Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4435f1-a81e-47bc-95b1-cef00b64585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88dcd-f0f2-4857-97d1-74ccdd0a6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Đường dẫn đến thư mục chứa các file CSV\n",
    "directory = './data'\n",
    "\n",
    "# Tìm tất cả các file CSV trong thư mục\n",
    "all_files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "\n",
    "# Đọc và gộp các file CSV thành một DataFrame\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "merged_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "# Lưu DataFrame gộp thành một file CSV\n",
    "merged_df.to_csv('all_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3759af-3b33-4fd3-aa27-286191580f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e15b9a-eaf9-4f2e-b7d9-d1644136bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b32d2-4c9c-4dc1-b77e-a2f894b578dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230cb983-6083-4234-b866-14c980f6dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset.columns)):\n",
    "    missing_data = dataset[dataset.columns[i]].isna().sum()\n",
    "    perc = missing_data / len(dataset) * 100\n",
    "    print('>%d,  missing entries: %d, percentage %.2f' % (i, missing_data, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4870ca-87a9-43e3-95b8-3cb9dedefd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,4)) #is to create a figure object with a given size\n",
    "sns.heatmap(dataset.isna(), cbar=False, cmap='viridis', yticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbdb66-1f02-45c4-9cdf-b49b15d6b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe into a numpy array by calling values on my dataframe (not necessary), but a habit I prefer\n",
    "X= dataset.iloc[:, 2:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9fb58-a383-4950-ba72-02d11bd6ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3edcb-1a81-4d7a-92bd-44863d499b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8efcd-d0aa-46aa-9ad3-5114a743e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "#output of fit_transform of Label Encoder is already a Numpy Array\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690d084-3778-41ec-acbb-ca19644a891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa38dd-d5c3-4675-8299-145b4d451704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be946189-accb-47cb-a313-f1f87bf4eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12024a5f-c8f1-4bec-927d-d6fde87ddd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d214f4-2654-4b0c-aed6-9166a2859732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e3f94-61f4-4b71-a8fe-7b2083ddd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b759c54-0b85-40d7-bce4-d8dfcd601f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,:] = sc.fit_transform(X_train[:,:])\n",
    "#only use Transform to use the SAME scaler as the Training Set\n",
    "X_test[:,:] = sc.transform(X_test[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed8ed9-1ab6-4e4d-afa2-6a714f7e3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc930541-0df3-4ba5-8a01-e33b9665fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e4a4c-950b-47e3-bc1c-0c28b41845e2",
   "metadata": {},
   "source": [
    "## 2.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ca44f-3b92-4b75-9f54-755b18298ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582e7df-f53e-45b4-bd31-21d24f6a0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\"Logistic Regression\": LogisticRegression(max_iter=10000), \n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"Decision Tree\": DecisionTreeClassifier(),\n",
    "          \"SVM\": SVC(),\n",
    "          \"GBM\": GradientBoostingClassifier(),\n",
    "          \"Naive Bayes\": GaussianNB()}\n",
    "\n",
    "# Creat a funciton to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different Scikit-Learn machine learning models\n",
    "    X_train : training data\n",
    "    X_test : testing data\n",
    "    y_train : labels assosciated with training data\n",
    "    y_test : labels assosciated with test data\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "    # Make a dictionary to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop throuhg models\n",
    "    for name, model in models.items():\n",
    "        #Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        #Evaluate the model and append its score to model_scores\n",
    "        model_scores[name]= model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6696f4-7c59-4ac9-9fbc-3c925f32b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_score = fit_and_score(models=models,\n",
    "                            X_train=X_train,\n",
    "                            X_test=X_test,\n",
    "                            y_train=y_train,\n",
    "                            y_test=y_test)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e71a61-d915-4fdb-bb5a-62140de35e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_score, index=[\"accuracy\"])\n",
    "model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac99fc-abf5-4217-a99c-dfb6671f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f970b10-5e46-46fd-8dce-601b3798bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a781ca8b-88d1-49de-bf0d-abccb71628c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyper-parameter grid for LogisticRegression()\n",
    "\n",
    "log_reg_grid = {\"C\": np.logspace(-4,4,20), #most valuable for Log Reg model\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Create a hyper-parameter grid for RandomForestClassifier()\n",
    "rf_grid = {\"n_estimators\": np.arange(10,1000,50),\n",
    "           \"max_depth\": [None, 3,5,10],\n",
    "           \"min_samples_split\": np.arange(2,20,2),\n",
    "           \"min_samples_leaf\": np.arange(1,20,2)}\n",
    "# Create a hyper-parameter grid for SVM\n",
    "svm_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']} \n",
    "\n",
    "# Create a hyper-parameter grid for GradientBoostingClassifier\n",
    "gbm_grid = {\n",
    "    'n_estimators': np.arange(50, 251, 50),\n",
    "    'learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "    'max_depth': np.arange(3, 8),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2502f-a5ac-48c2-a313-96d2b71fc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid = {\"C\": np.logspace(-4,4,20), \"solver\": [\"liblinear\"]}\n",
    "rf_grid = {\"n_estimators\": np.arange(10,1000,50), \"max_depth\": [None, 3,5,10], \"min_samples_split\": np.arange(2,20,2), \"min_samples_leaf\": np.arange(1,20,2)}\n",
    "svm_grid = {'C': np.logspace(-3, 3, 10), 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "gbm_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2], 'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3308e-4bd0-45c0-a825-cc9c2c462d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "params_grid = {\"Logistic Regression\": (LogisticRegression(max_iter=10000), log_reg_grid),\n",
    "               \"Random Forest\": (RandomForestClassifier(), rf_grid),\n",
    "               \"SVM\": (SVC(), svm_grid),\n",
    "               \"GBM\": (GradientBoostingClassifier(), gbm_grid)}\n",
    "\n",
    "# Creat a funciton to hyperparameter tuning with RandomizedSearchCV\n",
    "def tune(params_grid, X_train, X_test, y_train, y_test):\n",
    "    np.random.seed(42)\n",
    "    rs_scores = {}\n",
    "    for name, (model, param_grid) in params_grid.items():\n",
    "        rs = RandomizedSearchCV(model, param_distributions=param_grid, cv=5, n_iter=20, verbose=True, n_jobs=-1)\n",
    "        rs.fit(X_train, y_train)\n",
    "        rs_scores[name] = rs.score(X_test, y_test)\n",
    "    return rs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33582f37-eb0e-4fbf-b6b0-420b6dff3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_scores = tune(params_grid=params_grid,\n",
    "                            X_train=X_train,\n",
    "                            X_test=X_test,\n",
    "                            y_train=y_train,\n",
    "                            y_test=y_test)\n",
    "rs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d145dc-78f5-44b2-9d3c-8761e37956d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different hyperparameters for our LogisticRegression model\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33be84c-73bc-4286-b96c-373400b52bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the grid search LogisticRegression model\n",
    "gs_log_reg.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2947e5-7a89-4ae2-83fb-eb2470073fc0",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b4a05-012d-4b28-9e84-cfefa8ea0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score #Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea439dbd-fbd5-4b1b-882e-3b46aba13956",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = gs_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a64b0-3368-4e1f-a6ec-0729d52f27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf5dbc-fb81-4618-9f57-f57ee9206a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5) # Increase font size\n",
    " \n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False,\n",
    "                     cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n",
    "    plt.ylabel(\"True label\") # true labels go on the y-axis \n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83450ed1-f196-4d78-ac8a-48603d367850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_preds)) #Based on only 1 test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d843d9-cb8b-43a2-95f6-1c8e029c520d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "981d7c869dd2079483315c95b86c039a0bfcee0010f0840c07f83e4c693b861d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
